# ğŸ  House Prices - Advanced Regression Techniques
This project solves the Kaggle regression challenge by predicting final house sale prices using advanced preprocessing, feature engineering, and regression models like Lasso and Random Forest.

# ğŸ“‚ Dataset
Kaggle Competition: House Prices - Advanced Regression Techniques

Files used:

train.csv

test.csv

sample_submission.csv

# ğŸ§  Techniques Used
Data Cleaning (missing value imputation)

Feature Engineering:

Total Square Footage (TotalSF)

Total Bathrooms (TotalBath)

House Age and Remodelling Age

One-hot Encoding for categorical features

Regression Models:

LassoCV for regularized linear regression

Random Forest Regressor for ensemble tree learning

Model Evaluation using RMSE on log-transformed SalePrice

# ğŸ“Š Visualizations
Distribution of SalePrice (original & log-transformed)

Model residuals (Lasso)

Feature importance:

Lasso non-zero coefficients

Top 30 features from Random Forest

ğŸ“ˆ Model Performance (on training set)
Model	RMSE (log scale)
LassoCV	~0.12â€“0.15
Random Forest	~0.06â€“0.09

âš ï¸ Note: These are training RMSEs. For reliable generalization, cross-validation or test leaderboard is advised.

ğŸ“ File Structure
Copy
Edit
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ submission.csv
â”œâ”€â”€ house_price_regression.ipynb
â””â”€â”€ README.md
ğŸš€ How to Run
Clone the repository or open in Google Colab

Upload train.csv and test.csv

Run all cells to:

Preprocess data

Train Lasso and Random Forest

Visualize insights

Generate submission.csv

ğŸ› ï¸ Libraries Used
pandas

numpy

seaborn

matplotlib

scikit-learn

ğŸ“Œ Future Improvements
Use advanced models like XGBoost, LightGBM, or Stacking

Perform hyperparameter tuning

Add cross-validation RMSE

Experiment with feature selection techniques
